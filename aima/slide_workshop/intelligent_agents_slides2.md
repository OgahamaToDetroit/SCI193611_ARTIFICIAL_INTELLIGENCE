---
marp: true
theme: default
paginate: true
backgroundColor: #f8f9fa
style: |
  section {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
  }
  h1 {
    color: #2c3e50;
    border-bottom: 3px solid #3498db;
    padding-bottom: 10px;
  }
  h2 {
    color: #34495e;
    border-left: 4px solid #e74c3c;
    padding-left: 20px;
  }
  .highlight {
    background-color: #fff3cd;
    border: 1px solid #ffeaa7;
    border-radius: 5px;
    padding: 15px;
    margin: 10px 0;
  }
  code {
    background-color: #f4f4f4;
    padding: 2px 5px;
    border-radius: 3px;
    font-size: 0.9em;
  }
  .code-small {
    font-size: 0.75em;
  }
---

# ü§ñ Intelligent Agents Workshop
## AIMA Chapter 2: Intelligent Agents

**‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:**
- ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Agent ‡πÅ‡∏•‡∏∞ Environment
- ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡πÅ‡∏•‡∏∞ implement agent programs
- ‡∏•‡∏á‡∏°‡∏∑‡∏≠‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏™‡∏£‡πâ‡∏≤‡∏á agents ‡πÉ‡∏ô‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡∏ï‡πà‡∏≤‡∏á‡πÜ

**‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°:**
- Python environment ‡∏û‡∏£‡πâ‡∏≠‡∏° agents.py
- Jupyter Notebook ‡∏´‡∏£‡∏∑‡∏≠ Python IDE

---

## üìö ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç

1. **Overview - Agent ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?**
2. **Agent Architecture**
3. **Environment Types**
4. **Hands-on: BlindDog Agent**
5. **2D Environment & EnergeticBlindDog**
6. **Wumpus World Challenge**
7. **‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á**

---

## üéØ Agent ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?

<div class="highlight">

**Agent** = ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ:
- **‡∏£‡∏±‡∏ö‡∏£‡∏π‡πâ** ‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏° (perceive) ‡∏ú‡πà‡∏≤‡∏ô sensors
- **‡∏Å‡∏£‡∏∞‡∏ó‡∏≥** ‡∏ï‡πà‡∏≠‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏° (act) ‡∏ú‡πà‡∏≤‡∏ô actuators

</div>

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Agents:**
- üêï ‡∏™‡∏∏‡∏ô‡∏±‡∏Ç (sensors: ‡∏ï‡∏≤, ‡∏´‡∏π, ‡∏à‡∏°‡∏π‡∏Å | actuators: ‡∏Ç‡∏≤, ‡πÄ‡∏™‡∏µ‡∏¢‡∏á)
- ü§ñ ‡∏´‡∏∏‡πà‡∏ô‡∏¢‡∏ô‡∏ï‡πå (sensors: ‡∏Å‡∏•‡πâ‡∏≠‡∏á, ‡πÄ‡∏ã‡πá‡∏ô‡πÄ‡∏ã‡∏≠‡∏£‡πå | actuators: ‡∏°‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå, ‡πÅ‡∏Ç‡∏ô‡∏Å‡∏•)
- üíª Software Agent (sensors: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï | actuators: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏≠‡∏≤‡∏ï‡πå‡∏û‡∏∏‡∏ï)
- üë§ ‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå (sensors: ‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ó‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™ | actuators: ‡∏°‡∏∑‡∏≠, ‡πÄ‡∏ó‡πâ‡∏≤, ‡πÄ‡∏™‡∏µ‡∏¢‡∏á)

---
## üéØ Agent ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?

<div class="highlight">

**Agent** = ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ:
- **‡∏£‡∏±‡∏ö‡∏£‡∏π‡πâ** ‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏° (perceive) ‡∏ú‡πà‡∏≤‡∏ô **sensors**
- **‡∏Å‡∏£‡∏∞‡∏ó‡∏≥** ‡∏ï‡πà‡∏≠‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏° (act) ‡∏ú‡πà‡∏≤‡∏ô **actuators**

</div>

**‡πÅ‡∏ú‡∏ô‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:**
```
+-------------+   (Percepts)   +---------+   (Actions)     +-------------+
| Environment | <------------- |  Agent  | ------------>   | Environment |
|             | --[ Sensors ]->|         |--[ Actuators ]->|             |
+-------------+                +---------+                 +-------------+
```
**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:** ‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå, ‡∏´‡∏∏‡πà‡∏ô‡∏¢‡∏ô‡∏ï‡πå, Software Agent

---
## üèóÔ∏è Agent Architecture

```python
class Agent(Thing):
    def __init__(self, program=None):
        self.alive = True        # ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï
        self.bump = False        # ‡∏Å‡∏≤‡∏£‡∏ä‡∏ô‡∏Å‡∏≥‡πÅ‡∏û‡∏á
        self.holding = []        # ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ñ‡∏∑‡∏≠
        self.performance = 0     # ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
        self.program = program   # ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°

    def can_grab(self, thing):
        """Return True if this agent can grab this thing."""
        return False
```

**‡∏≠‡∏á‡∏Ñ‡πå‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:**
- **Program**: ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á percept ‚Üí action
- **Performance**: ‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
- **State**: ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Ç‡∏≠‡∏á agent

---

## üåç Environment Architecture

```python
class Environment:
    def __init__(self):
        self.things = []    # ‡∏™‡∏¥‡πà‡∏á‡∏Ç‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô environment
        self.agents = []    # agents ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î

    def percept(self, agent):
        """Return the percept that the agent sees."""
        raise NotImplementedError

    def execute_action(self, agent, action):
        """Change the world to reflect this action."""
        raise NotImplementedError
    
    def run(self, steps=1000):
        """Run the Environment for given number of time steps."""
        for step in range(steps):
            if self.is_done():
                return
            self.step()
```

---

## üêï ‡∏™‡∏£‡πâ‡∏≤‡∏á BlindDog Agent ‡πÅ‡∏£‡∏Å
‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå agents.ipynb:
```python
class BlindDog(Agent):
    def eat(self, thing):
        print("Dog: Ate food at {}.".format(self.location))
            
    def drink(self, thing):
        print("Dog: Drank water at {}.".format(self.location))

# ‡∏™‡∏£‡πâ‡∏≤‡∏á dog instance
dog = BlindDog()
print(dog.alive)  # True
```
---
**‡∏™‡∏∏‡∏ô‡∏±‡∏Ç‡∏ï‡∏≤‡∏ö‡∏≠‡∏î‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤:**
- üö´ ‡∏°‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏´‡πá‡∏ô ‡πÅ‡∏ï‡πà‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
- üçñ ‡∏Å‡∏¥‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÑ‡∏î‡πâ
- üíß ‡∏î‡∏∑‡πà‡∏°‡∏ô‡πâ‡∏≥‡πÑ‡∏î‡πâ
---
## üèûÔ∏è ‡∏™‡∏£‡πâ‡∏≤‡∏á Park Environment

```python
class Food(Thing):
    pass

class Water(Thing):
    pass

class Park(Environment):
    def percept(self, agent):
        '''return a list of things that are in our agent's location'''
        things = self.list_things_at(agent.location)
        return things
    
    def execute_action(self, agent, action):
        '''changes the state of the environment based on what the agent does.'''
        if action == "move down":
            print('{} decided to {} at location: {}'.format(
                str(agent)[1:-1], action, agent.location))
            agent.movedown()
        elif action == "eat":
            items = self.list_things_at(agent.location, tclass=Food)
            if len(items) != 0:
                if agent.eat(items[0]):
                    self.delete_thing(items[0])
        # ... drink logic
```
---

## üß† BlindDog Program Implementation

```python
class BlindDog(Agent):
    location = 1
    
    def movedown(self):
        self.location += 1
        
    def eat(self, thing):
        '''returns True upon success or False otherwise'''
        if isinstance(thing, Food):
            return True
        return False
    
    def drink(self, thing):
        ''' returns True upon success or False otherwise'''
        if isinstance(thing, Water):
            return True
        return False

def program(percepts):
    '''Returns an action based on the dog's percepts'''
    for p in percepts:
        if isinstance(p, Food):
            return 'eat'
        elif isinstance(p, Water):
            return 'drink'
    return 'move down'
```

---

## üéÆ Demo: BlindDog Simulation

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á environment ‡πÅ‡∏•‡∏∞ agent
park = Park()
dog = BlindDog(program)
dogfood = Food()
water = Water()

# ‡∏ß‡∏≤‡∏á‡∏™‡∏¥‡πà‡∏á‡∏Ç‡∏≠‡∏á‡πÉ‡∏ô‡∏™‡∏ß‡∏ô
park.add_thing(dog, 1)
park.add_thing(dogfood, 5)
park.add_thing(water, 7)

# ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á
park.run(5)
```

**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á:**
- ‡∏™‡∏∏‡∏ô‡∏±‡∏Ç‡πÄ‡∏î‡∏¥‡∏ô‡∏à‡∏≤‡∏Å‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á 1 ‚Üí 5 ‡πÅ‡∏•‡∏∞‡∏Å‡∏¥‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£
- ‡πÄ‡∏î‡∏¥‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á 7 ‡πÅ‡∏•‡∏∞‡∏î‡∏∑‡πà‡∏°‡∏ô‡πâ‡∏≥
===
**‡∏•‡∏≠‡∏á‡∏ó‡∏≥‡πÉ‡∏ô Jupyter Notebook ‡∏Å‡∏±‡∏ô!**

---

## üéØ Percept-Action Table

‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô notebook:

| **Percept** | **Action** |
|-------------|------------|
| Feel Food | eat |
| Feel Water | drink |
| Feel Nothing | move down |

```python
def program(percepts):
    '''Returns an action based on the dog's percepts'''
    for p in percepts:
        if isinstance(p, Food):
            return 'eat'
        elif isinstance(p, Water):
            return 'drink'
    return 'move down'
```
---
**Simple Reflex Agent**: ‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ï‡πà‡∏≠ percept

---

## üé® GraphicEnvironment - 2D Upgrade!

```python
class Park2D(GraphicEnvironment):
    def percept(self, agent):
        '''return a list of things that are in our agent's location'''
        things = self.list_things_at(agent.location)
        return things
    
    def execute_action(self, agent, action):
        if action == "move down":
            agent.movedown()
        elif action == "eat":
            items = self.list_things_at(agent.location, tclass=Food)
            if len(items) != 0:
                if agent.eat(items[0]):
                    self.delete_thing(items[0])
        # ... similar for drink

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏µ
park = Park2D(5, 20, color={
    'BlindDog': (200,0,0),      # ‡πÅ‡∏î‡∏á
    'Water': (0, 200, 200),     # ‡∏ü‡πâ‡∏≤  
    'Food': (230, 115, 40)      # ‡∏™‡πâ‡∏°
})
```

---
## üöÄ EnergeticBlindDog - 2D Movement
```python
class EnergeticBlindDog(Agent):
    location = [0,1]
    direction = Direction("down")
    def moveforward(self, success=True):
        if not success:
            return
        if self.direction.direction == Direction.R:
            self.location[0] += 1
        elif self.direction.direction == Direction.L:
            self.location[0] -= 1
        elif self.direction.direction == Direction.D:
            self.location[1] += 1
        elif self.direction.direction == Direction.U:
            self.location[1] -= 1
    def turn(self, d):
        self.direction = self.direction + d
```
---

## üé≤ EnergeticBlindDog Program

```python
from random import choice
import random

def program(percepts):
    '''Returns an action based on it's percepts'''
    
    for p in percepts: # first eat or drink - you're a dog!
        if isinstance(p, Food):
            return 'eat'
        elif isinstance(p, Water):
            return 'drink'
        if isinstance(p, Bump): # then check if you are at an edge
            choice = random.choice((1,2))  # turn only
        else:
            choice = random.choice((1,2,3,4)) # 1-right, 2-left, others-forward
    
    if choice == 1:
        return 'turnright'
    elif choice == 2:
        return 'turnleft'
    else:
        return 'moveforward'
```
---
**‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå:** ‡∏Å‡∏¥‡∏ô/‡∏î‡∏∑‡πà‡∏° ‚Üí ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≥‡πÅ‡∏û‡∏á ‚Üí ‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏´‡∏ß‡∏™‡∏∏‡πà‡∏°

---

## üó∫Ô∏è Enhanced Park2D Environment

```python
class Park2D(GraphicEnvironment):
    def percept(self, agent):
        things = self.list_things_at(agent.location)
        loc = copy.deepcopy(agent.location) # find target location
        
        # Check if agent is about to bump into a wall
        if agent.direction.direction == Direction.R:
            loc[0] += 1
        elif agent.direction.direction == Direction.L:
            loc[0] -= 1
        elif agent.direction.direction == Direction.D:
            loc[1] += 1
        elif agent.direction.direction == Direction.U:
            loc[1] -= 1
            
        if not self.is_inbounds(loc):
            things.append(Bump())
        return things
    
    def execute_action(self, agent, action):
        if action == 'turnright':
            agent.turn(Direction.R)
        elif action == 'turnleft':
            agent.turn(Direction.L)
        elif action == 'moveforward':
            agent.moveforward()
        # ... eat/drink logic
```

---

## üéÆ Demo: 2D EnergeticBlindDog

```python
park = Park2D(5, 5, color={
    'EnergeticBlindDog': (200,0,0), 
    'Water': (0, 200, 200), 
    'Food': (230, 115, 40)
})

dog = EnergeticBlindDog(program)
park.add_thing(dog, [0,0])
park.add_thing(Food(), [1,2])
park.add_thing(Water(), [0,1])
park.add_thing(Water(), [2,4])
park.add_thing(Food(), [4,3])

print("dog started at [0,0], facing down. Let's see if he found any food!")
park.run(20)
```

**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:** ‡∏™‡∏∏‡∏ô‡∏±‡∏Ç‡∏à‡∏∞‡∏™‡∏≥‡∏£‡∏ß‡∏à‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà 2D ‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°‡πÅ‡∏•‡∏∞‡∏´‡∏≤‡∏≠‡∏≤‡∏´‡∏≤‡∏£/‡∏ô‡πâ‡∏≥!

**‡∏•‡∏≠‡∏á‡∏£‡∏±‡∏ô‡∏î‡∏π!**

---

## üè∞ Wumpus World - The Ultimate Challenge!

‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå notebook:

```python
color = {"Breeze": (225, 225, 225),
        "Pit": (0,0,0),
        "Gold": (253, 208, 23),
        "Glitter": (253, 208, 23),
        "Wumpus": (43, 27, 23),
        "Stench": (128, 128, 128),
        "Explorer": (0, 0, 255),
        "Wall": (44, 53, 57)}

def program(percepts):
    '''Returns an action based on it's percepts'''
    print(percepts)
    return input()  # Manual control!

w = WumpusEnvironment(program, 7, 7)
```
---
**‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢:**
- üèÜ ‡∏´‡∏≤‡∏ó‡∏≠‡∏á‡∏Ñ‡∏≥‡πÅ‡∏•‡∏∞‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ñ‡πâ‡∏≥
- üíÄ ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á Wumpus ‡πÅ‡∏•‡∏∞ Pits
- üèπ ‡πÉ‡∏ä‡πâ‡∏•‡∏π‡∏Å‡∏®‡∏£‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏≤‡∏ç‡∏â‡∏•‡∏≤‡∏î

---

## üéÆ Wumpus World Interactive Demo

```python
from ipythonblocks import BlockGrid
w = WumpusEnvironment(program, 7, 7)         
grid = BlockGrid(w.width, w.height, fill=(123, 234, 123))

def draw_grid(world):
    global grid
    grid[:] = (123, 234, 123)
    for x in range(0, len(world)):
        for y in range(0, len(world[x])):
            if len(world[x][y]):
                grid[y, x] = color[world[x][y][-1].__class__.__name__]

def step():
    global grid, w
    draw_grid(w.get_world())
    grid.show()
    w.step()

# ‡πÉ‡∏ä‡πâ step() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡πà‡∏ô‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô
step()
```
---
**Actions ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ:** Forward, TurnLeft, TurnRight, Grab, Shoot, Climb

---

## üß† Wumpus World Percepts

**Format:** `[Left, Right, Up, Down, Center]`

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Percepts:**
- `Glitter`: ‡∏ó‡∏≠‡∏á‡∏Ñ‡∏≥‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
- `Stench`: Wumpus ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏Ç‡πâ‡∏≤‡∏á‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á
- `Breeze`: ‡∏´‡∏•‡∏∏‡∏°‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏Ç‡πâ‡∏≤‡∏á‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á
- `Bump`: ‡∏ä‡∏ô‡∏Å‡∏≥‡πÅ‡∏û‡∏á
- `Scream`: Wumpus ‡∏ñ‡∏π‡∏Å‡∏¢‡∏¥‡∏á‡πÅ‡∏•‡πâ‡∏ß

```python
# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á percepts ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö
[[None], [Stench], [None], [None], [Glitter]]
# ‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤: ‡∏ó‡∏≤‡∏á‡∏Ç‡∏ß‡∏≤‡∏°‡∏µ Stench, ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏°‡∏µ Glitter
```

---

## üí° Hands-on Exercise Time!

### üéØ ‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à 1: ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á BlindDog (15 ‡∏ô‡∏≤‡∏ó‡∏µ)
```python
class SmartBlindDog(Agent):
    def __init__(self, program):
        super().__init__(program)
        self.visited = set()    # ‡∏à‡∏≥‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏õ
        
def smart_program(percepts):
    # TODO: ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏¥‡∏°
    # TODO: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
    pass
```
---
### üéØ ‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à 2: Wumpus Agent Prototype (15 ‡∏ô‡∏≤‡∏ó‡∏µ)
```python
def safe_wumpus_program(percepts):
    # TODO: ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢
    # TODO: ‡∏´‡∏≤‡∏ó‡∏≠‡∏á‡∏Ñ‡∏≥‡πÅ‡∏•‡∏∞‡∏Å‡∏•‡∏±‡∏ö
    pass
```

**‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏•‡∏¢!**

---

## üîç Agent Performance Comparison

‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå agents.py:

```python
def compare_agents(EnvFactory, AgentFactories, n=10, steps=1000):
    """‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö agents ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÉ‡∏ô environments"""
    envs = [EnvFactory() for i in range(n)]
    return [(A, test_agent(A, steps, copy.deepcopy(envs)))
            for A in AgentFactories]

def test_agent(AgentFactory, steps, envs):
    """Return the mean score of running an agent"""
    def score(env):
        agent = AgentFactory()
        env.add_thing(agent)
        env.run(steps)
        return agent.performance
    
    return mean(map(score, envs))
```
---
**‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô:**
- üìà ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
- ‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ
- üéØ ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à

---

## ü§î Types of Agent Programs

### 1. **Simple Reflex Agent**
```python
def simple_reflex_program(percept):
    if percept == 'Dirty':
        return 'Suck'
    elif location == loc_A:
        return 'Right'
    else:
        return 'Left'
```
---
### 2. **Model-Based Reflex Agent** 
```python
model = {loc_A: None, loc_B: None}

def model_based_program(percept):
    location, status = percept
    model[location] = status  # Update the model
    if model[loc_A] == model[loc_B] == 'Clean':
        return 'NoOp'
    elif status == 'Dirty':
        return 'Suck'
    # ...
```

---

## üß™ Vacuum World Example

‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå agents.py:

```python
def ReflexVacuumAgent():
    def program(percept):
        location, status = percept
        if status == 'Dirty':
            return 'Suck'
        elif location == loc_A:
            return 'Right'
        elif location == loc_B:
            return 'Left'
    return Agent(program)

```
---

## üß™ Vacuum World Example
‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå agents.py:
```python
def ModelBasedVacuumAgent():
    model = {loc_A: None, loc_B: None}
    def program(percept):
        location, status = percept
        model[location] = status
        if model[loc_A] == model[loc_B] == 'Clean':
            return 'NoOp'
        elif status == 'Dirty':
            return 'Suck'
        elif location == loc_A:
            return 'Right'
        elif location == loc_B:
            return 'Left'
    return Agent(program)
```

---

## üéØ Design Principles

### üîÑ **PEAS Framework:**
- **P**erformance measure
- **E**nvironment  
- **A**ctuators
- **S**ensors
---
### üèóÔ∏è **Agent Architecture:**
- **Rationality**: ‡∏ó‡∏≥‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå
- **Autonomy**: ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏ô‡πÄ‡∏≠‡∏á‡∏à‡∏≤‡∏Å percepts
- **Adaptation**: ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
---
### üåç **Environment Properties:**
- **Observable**: ‡∏°‡∏≠‡∏á‡πÄ‡∏´‡πá‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏ï‡πá‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô
- **Deterministic**: ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏∏‡πà‡∏°
- **Episodic**: ‡πÅ‡∏ï‡πà‡∏•‡∏∞ action ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¥‡∏™‡∏£‡∏∞
- **Static**: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏á‡∏ó‡∏µ‡πà

---

## üöÄ Advanced Agent Types

### üéØ **Goal-Based Agent**
- ‡∏°‡∏µ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
- ‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ó‡∏≥
- ‡πÉ‡∏ä‡πâ Search algorithms
---
### ‚öñÔ∏è **Utility-Based Agent**  
- ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (utility function)
- ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
- ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ trade-offs
---
### üìö **Learning Agent**
- ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå
- ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
- Performance element + Learning element

---

## üîß Implementation Tips

### üíæ **‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö State:**
```python
class SmartAgent(Agent):
    def __init__(self, program):
        super().__init__(program)
        self.knowledge = {}     # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÇ‡∏•‡∏Å
        self.visited = set()    # ‡∏à‡∏≥‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏õ
        self.goals = []         # ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
```
---
### üéØ **‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô:**
```python
def planning_program(percepts):
    # 1. ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ
    update_knowledge(percepts)
    
    # 2. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå
    if immediate_action_needed():
        return urgent_action()
    
    # 3. ‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏´‡∏ß
    return plan_next_move()
```

---

## üéâ ‡∏™‡∏£‡∏∏‡∏õ Workshop
### ‚úÖ ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:
1. **‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î Agent-Environment**
   - Agent ‡∏£‡∏±‡∏ö‡∏£‡∏π‡πâ‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏∞‡∏ó‡∏≥‡∏ï‡πà‡∏≠‡∏™‡∏¥‡πà‡∏á‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°
   - Environment ‡πÉ‡∏´‡πâ percepts ‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á actions

2. **‡∏Å‡∏≤‡∏£ Implement ‡∏à‡∏£‡∏¥‡∏á**
   - BlindDog: Simple 1D agent
   - EnergeticBlindDog: 2D movement with graphics
   - WumpusWorld: Complex reasoning environment
---
## üéâ ‡∏™‡∏£‡∏∏‡∏õ Workshop
### ‚úÖ ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:
3. **Agent Program Patterns**
   - Simple Reflex vs Model-Based
   - Performance measurement
   - ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ state ‡πÅ‡∏•‡∏∞ knowledge
---

## üìö Code Examples Summary

### üêï **BlindDog (1D)**
```python
def program(percepts):
    for p in percepts:
        if isinstance(p, Food): return 'eat'
        elif isinstance(p, Water): return 'drink'
    return 'move down'
```

### üöÄ **EnergeticBlindDog (2D)**
```python
def program(percepts):
    for p in percepts:
        if isinstance(p, Food): return 'eat'
        elif isinstance(p, Water): return 'drink'
        if isinstance(p, Bump): 
            return random.choice(['turnright', 'turnleft'])
    return random.choice(['turnright', 'turnleft', 'moveforward', 'moveforward'])
```
---
### üè∞ **Wumpus World**
```python
def program(percepts):
    print(percepts)  # [Left, Right, Up, Down, Center]
    return input()   # Manual control for learning
```

---

## üöÄ Next Steps & Applications

### üìö **‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ï‡πà‡∏≠‡πÑ‡∏õ:**
- **Search Algorithms** (Chapter 3-4): ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á
- **Knowledge Representation** (Chapter 7-9): ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ logic
- **Machine Learning** (Chapter 18-21): agents ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ

### üíº **‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏à‡∏£‡∏¥‡∏á:**
- ü§ñ **Robotics**: ‡∏´‡∏∏‡πà‡∏ô‡∏¢‡∏ô‡∏ï‡πå‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î, ‡πÇ‡∏î‡∏£‡∏ô
- üéÆ **Game AI**: NPCs ‡∏ó‡∏µ‡πà‡∏â‡∏•‡∏≤‡∏î, ‡∏õ‡∏è‡∏¥‡∏Å‡∏¥‡∏£‡∏¥‡∏¢‡∏≤‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á
- üíª **Software Agents**: Chatbots, recommendation systems
- üè≠ **Automation**: ‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°
---
### üîó **Resources:**
- [AIMA Python Code](https://github.com/aofphy/SCI193611_ARTIFICIAL_INTELLIGENCE/tree/main/aima)
- [Jupyter Notebooks](https://github.com/aofphy/SCI193611_ARTIFICIAL_INTELLIGENCE/tree/main/agents.ipynb)

---

## üìù Homework & Projects

### üéØ **‡∏Å‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏ô (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 1 ‡∏Ç‡πâ‡∏≠):**

1. **Smart BlindDog**: ‡πÉ‡∏´‡πâ‡∏à‡∏≥‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡πÑ‡∏°‡πà‡∏Å‡∏•‡∏±‡∏ö
2. **Wumpus Solver**: ‡∏™‡∏£‡πâ‡∏≤‡∏á agent ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ logical reasoning
3. **Multi-Agent Park**: ‡∏™‡∏∏‡∏ô‡∏±‡∏Ç‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÅ‡∏Ç‡πà‡∏á‡∏Ç‡∏±‡∏ô‡∏´‡∏≤‡∏≠‡∏≤‡∏´‡∏≤‡∏£
---
### üèÜ **Project Ideas:**
- **Pac-Man AI**: Agent ‡∏Å‡∏¥‡∏ô‡∏à‡∏∏‡∏î‡∏´‡∏•‡∏µ‡∏Å‡∏ú‡∏µ
- **Trading Bot**: Agent ‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢‡∏´‡∏∏‡πâ‡∏ô‡∏à‡∏≥‡∏•‡∏≠‡∏á  
- **Smart Home Controller**: ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏ö‡πâ‡∏≤‡∏ô
- **Maze Solver**: ‡∏´‡∏≤‡∏ó‡∏≤‡∏á‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡πÄ‡∏Ç‡∏≤‡∏ß‡∏á‡∏Å‡∏ï

**‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡πà‡∏á: x ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå**
**‡∏™‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô:** Jupyter Notebook + ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏™‡∏≤‡∏ò‡∏¥‡∏ï

---

## ‚ùì Q&A Session
### ü§ù **‡∏ñ‡∏≤‡∏°-‡∏ï‡∏≠‡∏ö ‡πÅ‡∏•‡∏∞ Discussion**
**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à:**
- ‡∏ó‡∏≥‡πÑ‡∏° BlindDog ‡∏ñ‡∏∂‡∏á‡πÉ‡∏ä‡πâ Simple Reflex ‡πÑ‡∏î‡πâ?
- Agent ‡πÅ‡∏ö‡∏ö‡πÑ‡∏´‡∏ô‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡πÑ‡∏´‡∏ô?
- ‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ Agent ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?
- ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á AI Agent ‡∏Å‡∏±‡∏ö Program ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ?
**‡πÅ‡∏ö‡πà‡∏á‡∏õ‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå:**
- ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ implement
- ‡πÑ‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á agents
- ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á
**‡∏°‡∏≤‡∏Ñ‡∏∏‡∏¢‡∏Å‡∏±‡∏ô!** üó£Ô∏è

---
# üôè Thank You!
## Workshop: Intelligent Agents
### AIMA Chapter 2 - Hands-on Experience
**‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ:**
- ‚úÖ Agent & Environment concepts
- ‚úÖ Implementation patterns
- ‚úÖ ‡∏à‡∏≤‡∏Å BlindDog ‚Üí EnergeticBlindDog ‚Üí Wumpus World
- ‚úÖ Performance evaluation
**‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠:**
- üìß Email: [ittipon@g.sut.ac.th]
**Happy Coding!** üöÄü§ñ
---